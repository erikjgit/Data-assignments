{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba06a53-1133-43be-ac40-f370861067a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DAT405 Assignment 5 - Group 52\n",
    "<p>Hampus Jansson - (4 hrs)<p>\n",
    "<p>Erik Johannesen - (4 hrs)<p>\n",
    "    <p>May 9, 2023<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294cc03a-8629-4a52-98ad-3339bf73f088",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "# 1.1\n",
    "<p> One thing a model could base its prediction on is if certain key words have been used in the message by the sender. This could be words like; fortune, invest, crypto. Another factor could be that the sender's email adress is a known email adress that you have choosen previously to trust.<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba7673b-8dd7-4adf-b5e6-834cceb1b205",
   "metadata": {},
   "source": [
    "## 2.2 a)\n",
    "<p>CountVectorizer creates a matrix that shows the number of occurences of each unique token in a text. <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176006f1-0b72-4319-9c1a-125ae7275127",
   "metadata": {},
   "source": [
    "## 2.2 b)\n",
    "\n",
    "<p>MultinomialNB looks at how many times a feature appears and BernoulliNB only looks at whether a feature appears or not. It lools like MultinomialNB performs slightly better in all the tests done here.<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5335996-a836-45a9-9cac-17cccc9455b1",
   "metadata": {},
   "source": [
    "## 3.1\n",
    "<p>When the model is only trained on the easy hams, it performs well when tested on other easy hams.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce9987c3-752b-4ad8-86a9-47761c657e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score and confusion matrix for MultinomialNB: 0.9868938401048493\n",
      "[[1275    1]\n",
      " [  19  231]]\n",
      "\n",
      "Score and confusion matrix for BernoulliNB: 0.9344692005242464\n",
      "[[1270    6]\n",
      " [  94  156]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_ham = []\n",
    "test_ham = []\n",
    "classifications_train = []\n",
    "classifications_test = []\n",
    "hard_X= []\n",
    "hard_y =[]\n",
    "\n",
    "X=[]\n",
    "y=[]\n",
    "\n",
    "\n",
    "entries = os.listdir('C:\\\\Users\\\\eriko\\\\Documents\\\\Python kurs\\\\New folder\\\\uppgift4\\\\easy_ham')\n",
    "\n",
    "for i in range(0, len(entries)):\n",
    "    \n",
    "    with open('C:\\\\Users\\\\eriko\\\\Documents\\\\Python kurs\\\\New folder\\\\uppgift4\\\\easy_ham' + '\\\\' + entries[i]) as file:\n",
    "        text = file.read()\n",
    "        #train_ham.append(text)\n",
    "        classifications_train.append('ham')\n",
    "        X.append(text)\n",
    "        y.append('ham')\n",
    "\n",
    "\n",
    "\n",
    "entries = os.listdir('C:\\\\Users\\\\eriko\\\\Documents\\\\Python kurs\\\\New folder\\\\uppgift4\\\\spam')\n",
    "\n",
    "for i in range(0, len(entries)):\n",
    "    \n",
    "    with open('C:\\\\Users\\\\eriko\\\\Documents\\\\Python kurs\\\\New folder\\\\uppgift4\\\\spam' + '\\\\' + entries[i], encoding =\"iso-8859-1\") as file:\n",
    "        text = file.read()\n",
    "        #train_spam.append(text)\n",
    "        classifications_train.append('spam')\n",
    "        X.append(text)\n",
    "        y.append('spam')\n",
    "        hard_X.append(text)\n",
    "        hard_y.append('spam')\n",
    "  \n",
    "\n",
    "    \n",
    "#temp = train_ham\n",
    "#temp2 = test_ham\n",
    "    \n",
    "#for i in train_spam:\n",
    "#    temp.append(i)\n",
    "\n",
    "#for i in test_spam:\n",
    "#    temp2.append(i)\n",
    "    \n",
    "    \n",
    "vectorizer = CountVectorizer(max_features=40000)\n",
    "\n",
    "    \n",
    "vector_X = vectorizer.fit_transform(X).toarray() \n",
    "\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vector_X, y, test_size=0.5,random_state=0)\n",
    "\n",
    "mnb.fit(X_train, y_train)\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "mpred = mnb.predict(X_test)\n",
    "bpred = bnb.predict(X_test)\n",
    "\n",
    "print(\"Score and confusion matrix for MultinomialNB: \" + str(mnb.score(X_test, y_test)))\n",
    "      \n",
    "cm = confusion_matrix(y_test, mpred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nScore and confusion matrix for BernoulliNB: \" + str(bnb.score(X_test, y_test)))\n",
    "      \n",
    "cm = confusion_matrix(y_test, bpred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec1c14d-d0ed-4296-8f96-5538c75ffcde",
   "metadata": {},
   "source": [
    "## 3.1 continued\n",
    "\n",
    "<p>When the same model is tested on hard hams, it does not perform well and falsely classifies all the hard hams as spam.<p>\n",
    "<p>This might be because the hard hams have more features in common with the spam than the easy hams does.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "200a316b-ed34-4790-a828-d61a58ba9b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score and confusion matrix for MultinomialNB: 0.6644474034620506\n",
      "[[  0 250]\n",
      " [  2 499]]\n",
      "\n",
      "Score and confusion matrix for BernoulliNB: 0.6218375499334221\n",
      "[[  0 250]\n",
      " [ 34 467]]\n"
     ]
    }
   ],
   "source": [
    "entries = os.listdir('C:\\\\Users\\\\eriko\\\\Documents\\\\Python kurs\\\\New folder\\\\uppgift4\\\\hard_ham')\n",
    "\n",
    "for i in range(0, int(len(entries))):\n",
    "    \n",
    "    with open('C:\\\\Users\\\\eriko\\\\Documents\\\\Python kurs\\\\New folder\\\\uppgift4\\\\hard_ham' + '\\\\' + entries[i]) as file:\n",
    "        hard_X.append(text)\n",
    "        hard_y.append('ham')\n",
    "        X.append(text)\n",
    "        y.append('ham')\n",
    "\n",
    "hard_X = vectorizer.fit_transform(hard_X).toarray() \n",
    "        \n",
    "mpred = mnb.predict(hard_X)\n",
    "bpred = bnb.predict(hard_X)\n",
    "\n",
    "print(\"Score and confusion matrix for MultinomialNB: \" + str(mnb.score(hard_X, hard_y)))\n",
    "      \n",
    "cm = confusion_matrix(hard_y, mpred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nScore and confusion matrix for BernoulliNB: \" + str(bnb.score(hard_X, hard_y)))\n",
    "      \n",
    "cm = confusion_matrix(hard_y, bpred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39926b33-a65c-464d-a090-242d545b07ef",
   "metadata": {},
   "source": [
    "## 3.2\n",
    "<p>When the model is trained on both hard and easy hams, it performs approximately as well as the easy ham model did with easy hams.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c57e79e5-8ccc-4d76-bc5e-c77ce52d9a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score and confusion matrix for MultinomialNB: 0.9939430648092066\n",
      "[[1390    7]\n",
      " [   3  251]]\n",
      "\n",
      "Score and confusion matrix for BernoulliNB: 0.9672925499697154\n",
      "[[1387   10]\n",
      " [  44  210]]\n"
     ]
    }
   ],
   "source": [
    "vector_X = vectorizer.fit_transform(X).toarray() \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vector_X, y, test_size=0.5,random_state=0)\n",
    "\n",
    "mnb.fit(X_train, y_train)\n",
    "bnb.fit(X_train, y_train)\n",
    "\n",
    "mpred = mnb.predict(X_test)\n",
    "bpred = bnb.predict(X_test)\n",
    "\n",
    "print(\"Score and confusion matrix for MultinomialNB: \" + str(mnb.score(X_test, y_test)))\n",
    "      \n",
    "cm = confusion_matrix(y_test, mpred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nScore and confusion matrix for BernoulliNB: \" + str(bnb.score(X_test, y_test)))\n",
    "      \n",
    "cm = confusion_matrix(y_test, bpred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ab48e-034c-4f64-b7d6-2e60fc226d11",
   "metadata": {},
   "source": [
    "## 3.3\n",
    "\n",
    "<p>Because headers and footers were included in the text, there might have been many features that came from there rather than from the messages. Much of that information was likely irrelevant because much of it is the same regardless of whether a mail is spam or not. Removing that information might lead to better performance. <p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
